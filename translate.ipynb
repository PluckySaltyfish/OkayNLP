{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import fm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = '<start> ' + sent + ' <end>'\n",
    "    return sent\n",
    "def create_dataset(path,num_examples):\n",
    "    lines = fm.load_file(path)\n",
    "    lines = [preprocess(s) for s in lines[:num_examples]]\n",
    "    return lines\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\"\"\"字典编码，返回一个张量和一个字典\"\"\" \n",
    "\"\"\"查询:tokenizer.index_word[index]\"\"\" \n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post') #在后方加padding\n",
    "    return tensor, lang_tokenizer\n",
    "def load_dataset(tgt_path,src_path,num_examples=None):\n",
    "    tgt = list(create_dataset(tgt_path,num_examples))\n",
    "    src = list(create_dataset(src_path,num_examples))\n",
    "    tgt_tensor,tgt_tokenizer = tokenize(tgt)\n",
    "    src_tensor,src_tokenizer = tokenize(src)\n",
    "    return tgt_tensor,tgt_tokenizer,src_tensor,src_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制数据集大小\n",
    "num_example = 100000\n",
    "tgt_tensor,tgt_tokenizer,src_tensor,src_tokenizer = load_dataset('../../data/FBIS/fbis.zh','../../data/FBIS/fbis.en',num_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "max length: target - 4217 source -  483\n"
     ]
    }
   ],
   "source": [
    "print(len(tgt_tensor))\n",
    "max_len_tgt,max_len_src = max_length(tgt_tensor),max_length(src_tensor)\n",
    "print(\"max length: target -\",max_len_tgt,\"source - \",max_len_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 20000 80000 20000\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集\n",
    "src_train,src_val,tgt_train,tgt_val = train_test_split(src_tensor,tgt_tensor,test_size=0.2)\n",
    "print(len(src_train),len(src_val),len(tgt_train),len(tgt_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert(lan,tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print(\"%d ----> %s\" % (t,lan.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"创建tf.data数据集\"\"\"\n",
    "BUFFER_SIZE = len(src_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(src_train)#BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(src_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(tgt_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_train, tgt_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
