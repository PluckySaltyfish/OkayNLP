{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Evlk1N78HIXM",
    "outputId": "9ada0ad2-3297-414d-b7bb-748063302382"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "导入[bert-for-tf2]包使用bert，以及一些相关的加载预训练权重、tokenizing的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtI7cKWDbUVc"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaE2G_2DdzVg"
   },
   "source": [
    "to prepare/encode \n",
    "the data for feeding into our BERT model, by:\n",
    "  - tokenizing the text\n",
    "  - trim or pad it to a `max_seq_len` length\n",
    "  - append the special tokens `[CLS]` and `[SEP]`\n",
    "  - convert the string tokens to numerical `ID`s using the original model's token encoding from `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fm\n",
    "file_dir = '/home/tyx/data/split/usual/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fm.load_file(file_dir + 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22213"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "# from bert.tokenization import FullTokenizer\n",
    "from bert import bert_tokenization\n",
    "\n",
    "class data_loader:\n",
    "    def __init__(self, data_dir,tokenizer: bert_tokenization.FullTokenizer, sample_size=None, max_seq_len=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train = fm.load_file(data_dir + 'train.txt')\n",
    "        test = fm.load_file(data_dir + 'dev.txt')\n",
    "        if sample_size is not None:\n",
    "            assert sample_size % 128 == 0\n",
    "            train, test = train[:sample_size], test[:sample_size]\n",
    "        \n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, lst):\n",
    "        x, y = [], []\n",
    "        for line in lst:\n",
    "            text, label = line.split('\\t');\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "            x.append(token_ids)\n",
    "            y.append(int(label))\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGL0mEoNFGlP"
   },
   "source": [
    "## A tweak\n",
    "\n",
    "Because of a `tf.train.load_checkpoint` limitation requiring list permissions on the google storage bucket, we need to copy the pre-trained BERT weights locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "bert_ckpt_dir=\"../BERT/chinese_L-12_H-768_A-12/\"\n",
    "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
    "bert_config_file = bert_ckpt_dir + \"bert_config.json\"\n",
    "bert_model_dir=\"../BERT\"\n",
    "bert_model_name=\"chinese_L-12_H-768_A-12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "colab_type": "code",
    "id": "dGFfkWO07cWG",
    "outputId": "773f74c0-0f33-4626-929f-cf35c0996353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".model:\n",
      "total 12\n",
      "drwxrwxr-x. 3 tyx tyx 4096 Aug  2 16:59 .\n",
      "drwxrwxr-x. 6 tyx tyx 4096 Aug  5 15:57 ..\n",
      "drwxrwxr-x. 2 tyx tyx 4096 Aug  2 17:05 chinese_L-12_H-768_A-12\n",
      "\n",
      ".model/chinese_L-12_H-768_A-12:\n",
      "total 402908\n",
      "drwxrwxr-x. 2 tyx tyx      4096 Aug  2 17:05 .\n",
      "drwxrwxr-x. 3 tyx tyx      4096 Aug  2 16:59 ..\n",
      "-rw-r--r--. 1 tyx tyx       520 Aug  9 15:55 bert_config.json\n",
      "-rw-r--r--. 1 tyx tyx 411529768 Aug  9 15:55 bert_model.ckpt.data-00000-of-00001\n",
      "-rw-r--r--. 1 tyx tyx      8512 Aug  9 15:55 bert_model.ckpt.index\n",
      "-rw-r--r--. 1 tyx tyx    905069 Aug  9 15:55 bert_model.ckpt.meta\n",
      "-rw-r--r--. 1 tyx tyx    109540 Aug  9 15:55 vocab.txt\n",
      "CPU times: user 149 ms, sys: 1.3 s, total: 1.44 s\n",
      "Wall time: 9.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "!mkdir -p .model .model/$bert_model_name\n",
    "\n",
    "for fname in [\"bert_config.json\", \"vocab.txt\", \"bert_model.ckpt.meta\", \"bert_model.ckpt.index\", \"bert_model.ckpt.data-00000-of-00001\"]:\n",
    "  cmd = f\"cp {bert_model_dir}/{bert_model_name}/{fname} .model/{bert_model_name}\"\n",
    "  !$cmd\n",
    "!ls -la .model .model/$bert_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "049feT8dFprc"
   },
   "outputs": [],
   "source": [
    "bert_ckpt_dir    = os.path.join(\".model/\",bert_model_name)\n",
    "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4xPTleh2X2b"
   },
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Now let's fetch and prepare the data by taking the first `max_seq_len` tokenens after tokenizing with the BERT tokenizer, und use `sample_size` examples for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use `max_seq_len=512`, but on a GPU this would be too slow, and you will have to use a very small `batch_size`s to fit the model into the GPU memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "kF_3KhGQ0GTc",
    "outputId": "fd993d23-61ce-4aae-c992-5b5591123198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 192\n",
      "CPU times: user 15.9 s, sys: 132 ms, total: 16 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "data = data_loader(file_dir,tokenizer, \n",
    "                       sample_size=None,#5000, \n",
    "                       max_seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "b98433d4-c7e6-4bb5-af54-941f52e0b8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            train_x (22213, 192)\n",
      "train_x_token_types (22213, 192)\n",
      "            train_y (22213,)\n",
      "             test_x (5553, 192)\n",
      "        max_seq_len 192\n"
     ]
    }
   ],
   "source": [
    "print(\"            train_x\", data.train_x.shape)\n",
    "print(\"train_x_token_types\", data.train_x_token_types.shape)\n",
    "print(\"            train_y\", data.train_y.shape)\n",
    "print(\"             test_x\", data.test_x.shape)\n",
    "print(\"        max_seq_len\", data.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "## Adapter BERT\n",
    "\n",
    "If we decide to use [adapter-BERT](https://arxiv.org/abs/1902.00751) we need some helpers for freezing the original BERT layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "\n",
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now let's create a classification model using [adapter-BERT](https//arxiv.org/abs/1902.00751), which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. `adapter_size` bellow) in every BERT layer.\n",
    "\n",
    "**N.B.** The commented out code below show how to feed a `token_type_ids`/`segment_ids` sequence (which is not needed in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, adapter_size=64):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    #adapter_size = 64  # see - arXiv:1902.00751\n",
    "\n",
    "    # create the bert layer\n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = adapter_size\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "    input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "    # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
    "    # output         = bert([input_ids, token_type_ids])\n",
    "    output         = bert(input_ids)\n",
    "\n",
    "    print(\"bert shape\", output.shape)\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    logits = keras.layers.Dense(units=6, activation=\"softmax\")(logits)\n",
    "\n",
    "    # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
    "    # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
    "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "    # load the pre-trained model weights\n",
    "    load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "    # freeze weights if adapter-BERT is used\n",
    "    if adapter_size is not None:\n",
    "        freeze_bert_layers(bert)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "bZnmtDc7HlEm",
    "outputId": "fcd96c78-792c-4032-d188-73a9c21ec304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 192, 768)\n",
      "loader: No value for:[bert_3/encoder/layer_0/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_0/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_0/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_0/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_0/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_0/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_0/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_0/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_0/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_0/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_0/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_0/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_1/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_1/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_1/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_1/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_1/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_2/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_2/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_2/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_2/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_2/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_3/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_3/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_3/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_3/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_3/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_4/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_4/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_4/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_4/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_4/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_5/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_5/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_5/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_5/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_5/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_6/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_6/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_6/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_6/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_6/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_7/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_7/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_7/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_7/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_7/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_8/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_8/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_8/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_8/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_8/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_9/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_9/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_9/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_9/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_9/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_10/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_10/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_10/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_10/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_10/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_11/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_11/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_11/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_3/encoder/layer_11/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_11/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "Done loading 196 BERT weights from: .model/chinese_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f4fd412a690> (prefix:bert_3). Count of weights not found in the checkpoint was: [96]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 192, 768)          101730840 \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 4614      \n",
      "=================================================================\n",
      "Total params: 102,326,046\n",
      "Trainable params: 687,390\n",
      "Non-trainable params: 101,638,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adapter_size = 1 # use None to fine-tune all of BERT\n",
    "model = create_model(data.max_seq_len, adapter_size=adapter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZuLOkwonF-9S",
    "outputId": "ce1451d4-310c-41f1-ddd3-a2e0841d8528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19991 samples, validate on 2222 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 2.5e-06.\n",
      "Epoch 1/50\n",
      "19991/19991 [==============================] - 485s 24ms/sample - loss: 1.7486 - acc: 0.2628 - val_loss: 1.7126 - val_acc: 0.3060\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5e-06.\n",
      "Epoch 2/50\n",
      "19991/19991 [==============================] - 478s 24ms/sample - loss: 1.6858 - acc: 0.3442 - val_loss: 1.5815 - val_acc: 0.4662\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 7.500000000000001e-06.\n",
      "Epoch 3/50\n",
      "19991/19991 [==============================] - 465s 23ms/sample - loss: 1.5714 - acc: 0.4750 - val_loss: 1.4509 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 4/50\n",
      "19991/19991 [==============================] - 479s 24ms/sample - loss: 1.4705 - acc: 0.5777 - val_loss: 1.3832 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 1.25e-05.\n",
      "Epoch 5/50\n",
      "19991/19991 [==============================] - 481s 24ms/sample - loss: 1.4233 - acc: 0.6220 - val_loss: 1.3688 - val_acc: 0.6710\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 1.5000000000000002e-05.\n",
      "Epoch 6/50\n",
      "19991/19991 [==============================] - 481s 24ms/sample - loss: 1.4036 - acc: 0.6381 - val_loss: 1.3610 - val_acc: 0.6791\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 1.7500000000000002e-05.\n",
      "Epoch 7/50\n",
      "19991/19991 [==============================] - 479s 24ms/sample - loss: 1.3902 - acc: 0.6514 - val_loss: 1.3551 - val_acc: 0.6854\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 8/50\n",
      "19991/19991 [==============================] - 472s 24ms/sample - loss: 1.3834 - acc: 0.6578 - val_loss: 1.3511 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 2.25e-05.\n",
      "Epoch 9/50\n",
      "19991/19991 [==============================] - 481s 24ms/sample - loss: 1.3750 - acc: 0.6649 - val_loss: 1.3431 - val_acc: 0.6994\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 2.5e-05.\n",
      "Epoch 10/50\n",
      "19991/19991 [==============================] - 485s 24ms/sample - loss: 1.3729 - acc: 0.6661 - val_loss: 1.3393 - val_acc: 0.7021\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 2.75e-05.\n",
      "Epoch 11/50\n",
      "19991/19991 [==============================] - 487s 24ms/sample - loss: 1.3623 - acc: 0.6783 - val_loss: 1.3298 - val_acc: 0.7124\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 3.0000000000000004e-05.\n",
      "Epoch 12/50\n",
      "19991/19991 [==============================] - 477s 24ms/sample - loss: 1.3471 - acc: 0.6945 - val_loss: 1.3165 - val_acc: 0.7232\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 3.2500000000000004e-05.\n",
      "Epoch 13/50\n",
      "19991/19991 [==============================] - 487s 24ms/sample - loss: 1.3425 - acc: 0.6978 - val_loss: 1.3112 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 3.5000000000000004e-05.\n",
      "Epoch 14/50\n",
      "19991/19991 [==============================] - 486s 24ms/sample - loss: 1.3340 - acc: 0.7076 - val_loss: 1.3045 - val_acc: 0.7349\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 3.7500000000000003e-05.\n",
      "Epoch 15/50\n",
      "19991/19991 [==============================] - 487s 24ms/sample - loss: 1.3235 - acc: 0.7183 - val_loss: 1.2913 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "Epoch 16/50\n",
      "19991/19991 [==============================] - 485s 24ms/sample - loss: 1.3174 - acc: 0.7248 - val_loss: 1.2954 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 4.25e-05.\n",
      "Epoch 17/50\n",
      "19991/19991 [==============================] - 478s 24ms/sample - loss: 1.3090 - acc: 0.7310 - val_loss: 1.2823 - val_acc: 0.7579\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 4.5e-05.\n",
      "Epoch 18/50\n",
      "19991/19991 [==============================] - 486s 24ms/sample - loss: 1.3049 - acc: 0.7365 - val_loss: 1.2756 - val_acc: 0.7651\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4.75e-05.\n",
      "Epoch 19/50\n",
      "10288/19991 [==============>...............] - ETA: 3:43 - loss: 1.3028 - acc: 0.7380"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "log_dir = \".log/bert_origin/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "total_epoch_count = 50\n",
    "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
    "model.fit(x=data.train_x, y=data.train_y,\n",
    "          validation_split=0.1,\n",
    "          batch_size=16,\n",
    "          shuffle=True,\n",
    "          epochs=total_epoch_count,\n",
    "          callbacks=[create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                                    end_learn_rate=1e-6,\n",
    "                                                    warmup_epoch_count=20,\n",
    "                                                    total_epoch_count=total_epoch_count),\n",
    "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "                     tensorboard_callback])\n",
    "\n",
    "model.save_weights('./models/bert'+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\") +'.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "BSqMu64oHzqy",
    "outputId": "95a8284b-b3b2-4a7d-f335-c4ff65f8f920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208/6871 [========>.....................] - ETA: 54s - loss: 1.8369 - acc: 0.0643"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSKDZEnVabnl"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the trained model, let's load the saved weights in a new model instance, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "qCpabQ15WS3U",
    "outputId": "220b2b55-dd78-4795-d3b6-d364c8323734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 200, 768)\n",
      "loader: No value for:[bert_1/encoder/layer_0/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_0/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_0/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_0/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_0/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_0/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_0/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_0/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_0/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_0/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_0/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_0/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_0/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_1/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_1/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_1/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_1/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_1/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_1/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_2/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_2/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_2/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_2/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_2/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_2/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_3/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_3/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_3/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_3/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_3/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_3/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_4/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_4/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_4/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_4/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_4/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_4/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_5/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_5/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_5/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_5/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_5/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_5/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_6/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_6/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_6/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_6/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_6/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_6/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_7/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_7/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_7/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_7/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_7/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_7/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_8/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_8/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_8/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_8/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_8/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_8/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_9/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_9/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_9/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_9/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_9/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_9/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_10/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_10/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_10/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_10/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_10/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_10/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/attention/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/attention/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/attention/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/attention/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_11/attention/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/output/adapter-down/kernel:0], i.e.:[bert/encoder/layer_11/output/adapter-down/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/output/adapter-down/bias:0], i.e.:[bert/encoder/layer_11/output/adapter-down/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/output/adapter-up/kernel:0], i.e.:[bert/encoder/layer_11/output/adapter-up/kernel] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "loader: No value for:[bert_1/encoder/layer_11/output/adapter-up/bias:0], i.e.:[bert/encoder/layer_11/output/adapter-up/bias] in:[.model/chinese_L-12_H-768_A-12/bert_model.ckpt]\n",
      "Done loading 196 BERT weights from: .model/chinese_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f51346df550> (prefix:bert_1). Count of weights not found in the checkpoint was: [96]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 200, 768)          101730840 \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 4614      \n",
      "=================================================================\n",
      "Total params: 102,326,046\n",
      "Trainable params: 687,390\n",
      "Non-trainable params: 101,638,656\n",
      "_________________________________________________________________\n",
      "6871/6871 [==============================] - 69s 10ms/sample - loss: 1.1955 - acc: 0.8467\n",
      "1722/1722 [==============================] - 16s 9ms/sample - loss: 1.2592 - acc: 0.7811\n",
      "train acc 0.8467472\n",
      " test acc 0.7810685\n",
      "CPU times: user 52.4 s, sys: 6.05 s, total: 58.4 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model = create_model(data.max_seq_len, adapter_size=1)\n",
    "model.load_weights(\"movie_reviews.h5\")\n",
    "\n",
    "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uzdOFQ5awM1"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special `[CLS]` and `[SEP]` token at begin and end of the token sequence, and pad to match the model input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "m7dAAoCuW1xj",
    "outputId": "15f4a565-1659-4c7a-a21a-5a1322389746"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "pred_tokens    = map(tokenizer.tokenize, pred_sentences)\n",
    "pred_tokens    = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "\n",
    "pred_token_ids = map(lambda tids: tids +[0]*(data.max_seq_len-len(tids)),pred_token_ids)\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "\n",
    "print('pred_token_ids', pred_token_ids.shape)\n",
    "\n",
    "res = model.predict(pred_token_ids).argmax(axis=-1)\n",
    "\n",
    "for text, sentiment in zip(pred_sentences, res):\n",
    "  print(\" text:\", text)\n",
    "  print(\"  res:\", [\"negative\",\"positive\"][sentiment])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Movie Reviews with bert-for-tf2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
